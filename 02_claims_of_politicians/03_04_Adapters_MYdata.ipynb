{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfb8515f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "from gensim.models import Doc2Vec\n",
    "\n",
    "import gensim\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from sklearn.base import clone as sklearn_clone\n",
    "\n",
    "from datasets import Dataset, DatasetDict\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from transformers import BertAdapterModel, BertConfig\n",
    "\n",
    "from transformers import PfeifferConfig\n",
    "\n",
    "from transformers import TrainingArguments, AdapterTrainer, EvalPrediction\n",
    "from sklearn.metrics import accuracy_score, f1_score, mean_absolute_error\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fe22cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/58497442/best-training-methods-for-binary-text-classification-using-doc2vec-gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142f7705",
   "metadata": {},
   "source": [
    "## Lodad data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a313870a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6541, 1)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_topics = pd.read_csv('../datasets/ready2use/topics.csv', index_col=0)\n",
    "df_topics.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a70584dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6541, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../datasets/ready2use/text_celan_pl_dataset.csv',\n",
    "                 index_col=0,\n",
    "                 sep=';',header=None,names='sentence labels'.split())\n",
    "\n",
    "df['sentence'] = df['sentence'].apply(lambda x: x.replace('\\n',' ').strip())\n",
    "df['labels'] = df['labels'].astype(int)\n",
    "\n",
    "df = df.sample(frac=1, random_state=111)\n",
    "\n",
    "df = df[df.index.isin(df_topics.index)]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "812cec5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = AutoTokenizer.from_pretrained('allegro/herbert-large-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c996543",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = sorted(set(df['labels']))\n",
    "num_labels = len(labels)\n",
    "label2id = dict(zip(labels,range(num_labels)))\n",
    "id2label = {v: k for k, v in label2id.items()}\n",
    "\n",
    "def encode_batch(batch):\n",
    "    \"\"\"Encodes a batch of input data using the model tokenizer.\"\"\"\n",
    "    batch['labels'] = np.array(batch['labels'])\n",
    "    return tok(text=batch['sentence'], max_length=64, truncation=True, padding=\"max_length\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4359e4c4",
   "metadata": {},
   "source": [
    "### Kfold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e02cc32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_fold = []\n",
    "cv_fold_i = []\n",
    "\n",
    "for i in df_topics['topic'].unique().reshape(10,-1):\n",
    "    train_cv = df_topics.index[ ~np.isin(df_topics[\"topic\"], i) ].values\n",
    "    test_cv = df_topics.index[ np.isin(df_topics[\"topic\"], i) ].values\n",
    "    \n",
    "    train_cv_i = df_topics.reset_index().index[ ~np.isin(df_topics[\"topic\"], i) ].values\n",
    "    test_cv_i = df_topics.reset_index().index[ np.isin(df_topics[\"topic\"], i) ].values\n",
    "    \n",
    "    cv_fold.append( [train_cv, test_cv])\n",
    "    cv_fold_i.append( [train_cv_i, test_cv_i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4208d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=10, shuffle=True)\n",
    "kf.get_n_splits(df_topics)\n",
    "\n",
    "cv_Kfold = []\n",
    "cv_Kfold_i = []\n",
    "\n",
    "for train_index, test_index in kf.split(df_topics):\n",
    "    train_cv = df_topics.iloc[ train_index, : ].index.values\n",
    "    test_cv = df_topics.iloc[ test_index, : ].index.values\n",
    "\n",
    "    train_cv_i= df_topics.reset_index().iloc[ train_index, : ].index.values\n",
    "    test_cv_i = df_topics.reset_index().iloc[ test_index, : ].index.values\n",
    "    \n",
    "    cv_Kfold.append( [train_cv, test_cv])\n",
    "    cv_Kfold_i.append( [train_cv_i, test_cv_i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b126bc0",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb896259",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = BertConfig.from_pretrained(\n",
    "    'allegro/herbert-large-cased',\n",
    "    num_labels=num_labels\n",
    ")\n",
    "\n",
    "adapter_config = PfeifferConfig()\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    remove_unused_columns=False, # ensure the dataset labels are properly passed to the model\n",
    "    \n",
    "    fp16=True,\n",
    "    learning_rate=1e-4,\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=32,\n",
    "    gradient_accumulation_steps=1,\n",
    "    max_grad_norm=1.0,\n",
    "    label_smoothing_factor=0.0,\n",
    "    weight_decay=0.05,\n",
    "    warmup_steps=600,\n",
    "    output_dir=\"./training_output\",\n",
    "    overwrite_output_dir=True,\n",
    "    \n",
    "#     logging_strategy=\"no\"\n",
    "    \n",
    "#     logging_steps=200,\n",
    "#     logging_first_step=False,\n",
    "#     logging_dir='./tb_logs',\n",
    "#     evaluation_strategy='steps',\n",
    "#     report_to='tensorboard',\n",
    "#     save_steps=200\n",
    ")\n",
    "\n",
    "def compute_accuracy_f1score(p: EvalPrediction):\n",
    "    preds = [id2label[pred_id] for pred_id in np.argmax(p.predictions, axis=1)]\n",
    "    target_labels = [id2label[label_id] for label_id in p.label_ids]\n",
    "    return {\"accuracy\": accuracy_score(target_labels,preds),\n",
    "           \"f1score\": f1_score(target_labels,preds)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb9dae2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = cv_fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af4ed52",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10d73d3aa01f41e099d1ab8859007940",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e37814d97fd4f47b8fa673c18abe070",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allegro/herbert-large-cased were not used when initializing BertAdapterModel: ['cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.sso.sso_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.sso.sso_relationship.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertAdapterModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertAdapterModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Using cuda_amp half precision backend\n",
      "/home/marek/anaconda3/envs/nlp/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 5946\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 7440\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7440' max='7440' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7440/7440 14:56, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.663800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.602900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.560500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.533500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.516900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.493500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.459300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.434200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.405300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.370800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.355700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.336100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.302400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.313500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./training_output/checkpoint-500\n",
      "Configuration saved in ./training_output/checkpoint-500/fake_news_1/adapter_config.json\n",
      "Module weights saved in ./training_output/checkpoint-500/fake_news_1/pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/checkpoint-500/fake_news_1/head_config.json\n",
      "Module weights saved in ./training_output/checkpoint-500/fake_news_1/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/checkpoint-500/fake_news_1/head_config.json\n",
      "Module weights saved in ./training_output/checkpoint-500/fake_news_1/pytorch_model_head.bin\n",
      "Saving model checkpoint to ./training_output/checkpoint-1000\n",
      "Configuration saved in ./training_output/checkpoint-1000/fake_news_1/adapter_config.json\n",
      "Module weights saved in ./training_output/checkpoint-1000/fake_news_1/pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/checkpoint-1000/fake_news_1/head_config.json\n",
      "Module weights saved in ./training_output/checkpoint-1000/fake_news_1/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/checkpoint-1000/fake_news_1/head_config.json\n",
      "Module weights saved in ./training_output/checkpoint-1000/fake_news_1/pytorch_model_head.bin\n",
      "Saving model checkpoint to ./training_output/checkpoint-1500\n",
      "Configuration saved in ./training_output/checkpoint-1500/fake_news_1/adapter_config.json\n",
      "Module weights saved in ./training_output/checkpoint-1500/fake_news_1/pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/checkpoint-1500/fake_news_1/head_config.json\n",
      "Module weights saved in ./training_output/checkpoint-1500/fake_news_1/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/checkpoint-1500/fake_news_1/head_config.json\n",
      "Module weights saved in ./training_output/checkpoint-1500/fake_news_1/pytorch_model_head.bin\n",
      "Saving model checkpoint to ./training_output/checkpoint-2000\n",
      "Configuration saved in ./training_output/checkpoint-2000/fake_news_1/adapter_config.json\n",
      "Module weights saved in ./training_output/checkpoint-2000/fake_news_1/pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/checkpoint-2000/fake_news_1/head_config.json\n",
      "Module weights saved in ./training_output/checkpoint-2000/fake_news_1/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/checkpoint-2000/fake_news_1/head_config.json\n",
      "Module weights saved in ./training_output/checkpoint-2000/fake_news_1/pytorch_model_head.bin\n",
      "Saving model checkpoint to ./training_output/checkpoint-2500\n",
      "Configuration saved in ./training_output/checkpoint-2500/fake_news_1/adapter_config.json\n",
      "Module weights saved in ./training_output/checkpoint-2500/fake_news_1/pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/checkpoint-2500/fake_news_1/head_config.json\n",
      "Module weights saved in ./training_output/checkpoint-2500/fake_news_1/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/checkpoint-2500/fake_news_1/head_config.json\n",
      "Module weights saved in ./training_output/checkpoint-2500/fake_news_1/pytorch_model_head.bin\n",
      "Saving model checkpoint to ./training_output/checkpoint-3000\n",
      "Configuration saved in ./training_output/checkpoint-3000/fake_news_1/adapter_config.json\n",
      "Module weights saved in ./training_output/checkpoint-3000/fake_news_1/pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/checkpoint-3000/fake_news_1/head_config.json\n",
      "Module weights saved in ./training_output/checkpoint-3000/fake_news_1/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/checkpoint-3000/fake_news_1/head_config.json\n",
      "Module weights saved in ./training_output/checkpoint-3000/fake_news_1/pytorch_model_head.bin\n",
      "Saving model checkpoint to ./training_output/checkpoint-3500\n",
      "Configuration saved in ./training_output/checkpoint-3500/fake_news_1/adapter_config.json\n",
      "Module weights saved in ./training_output/checkpoint-3500/fake_news_1/pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/checkpoint-3500/fake_news_1/head_config.json\n",
      "Module weights saved in ./training_output/checkpoint-3500/fake_news_1/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/checkpoint-3500/fake_news_1/head_config.json\n",
      "Module weights saved in ./training_output/checkpoint-3500/fake_news_1/pytorch_model_head.bin\n",
      "Saving model checkpoint to ./training_output/checkpoint-4000\n",
      "Configuration saved in ./training_output/checkpoint-4000/fake_news_1/adapter_config.json\n",
      "Module weights saved in ./training_output/checkpoint-4000/fake_news_1/pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/checkpoint-4000/fake_news_1/head_config.json\n",
      "Module weights saved in ./training_output/checkpoint-4000/fake_news_1/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/checkpoint-4000/fake_news_1/head_config.json\n",
      "Module weights saved in ./training_output/checkpoint-4000/fake_news_1/pytorch_model_head.bin\n",
      "Saving model checkpoint to ./training_output/checkpoint-4500\n",
      "Configuration saved in ./training_output/checkpoint-4500/fake_news_1/adapter_config.json\n",
      "Module weights saved in ./training_output/checkpoint-4500/fake_news_1/pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/checkpoint-4500/fake_news_1/head_config.json\n",
      "Module weights saved in ./training_output/checkpoint-4500/fake_news_1/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/checkpoint-4500/fake_news_1/head_config.json\n",
      "Module weights saved in ./training_output/checkpoint-4500/fake_news_1/pytorch_model_head.bin\n",
      "Saving model checkpoint to ./training_output/checkpoint-5000\n",
      "Configuration saved in ./training_output/checkpoint-5000/fake_news_1/adapter_config.json\n",
      "Module weights saved in ./training_output/checkpoint-5000/fake_news_1/pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/checkpoint-5000/fake_news_1/head_config.json\n",
      "Module weights saved in ./training_output/checkpoint-5000/fake_news_1/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/checkpoint-5000/fake_news_1/head_config.json\n",
      "Module weights saved in ./training_output/checkpoint-5000/fake_news_1/pytorch_model_head.bin\n",
      "Saving model checkpoint to ./training_output/checkpoint-5500\n",
      "Configuration saved in ./training_output/checkpoint-5500/fake_news_1/adapter_config.json\n",
      "Module weights saved in ./training_output/checkpoint-5500/fake_news_1/pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/checkpoint-5500/fake_news_1/head_config.json\n",
      "Module weights saved in ./training_output/checkpoint-5500/fake_news_1/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/checkpoint-5500/fake_news_1/head_config.json\n",
      "Module weights saved in ./training_output/checkpoint-5500/fake_news_1/pytorch_model_head.bin\n",
      "Saving model checkpoint to ./training_output/checkpoint-6000\n",
      "Configuration saved in ./training_output/checkpoint-6000/fake_news_1/adapter_config.json\n",
      "Module weights saved in ./training_output/checkpoint-6000/fake_news_1/pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/checkpoint-6000/fake_news_1/head_config.json\n",
      "Module weights saved in ./training_output/checkpoint-6000/fake_news_1/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/checkpoint-6000/fake_news_1/head_config.json\n",
      "Module weights saved in ./training_output/checkpoint-6000/fake_news_1/pytorch_model_head.bin\n",
      "Saving model checkpoint to ./training_output/checkpoint-6500\n",
      "Configuration saved in ./training_output/checkpoint-6500/fake_news_1/adapter_config.json\n",
      "Module weights saved in ./training_output/checkpoint-6500/fake_news_1/pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/checkpoint-6500/fake_news_1/head_config.json\n",
      "Module weights saved in ./training_output/checkpoint-6500/fake_news_1/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/checkpoint-6500/fake_news_1/head_config.json\n",
      "Module weights saved in ./training_output/checkpoint-6500/fake_news_1/pytorch_model_head.bin\n",
      "Saving model checkpoint to ./training_output/checkpoint-7000\n",
      "Configuration saved in ./training_output/checkpoint-7000/fake_news_1/adapter_config.json\n",
      "Module weights saved in ./training_output/checkpoint-7000/fake_news_1/pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/checkpoint-7000/fake_news_1/head_config.json\n",
      "Module weights saved in ./training_output/checkpoint-7000/fake_news_1/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/checkpoint-7000/fake_news_1/head_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Module weights saved in ./training_output/checkpoint-7000/fake_news_1/pytorch_model_head.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 595\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='19' max='19' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [19/19 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f75c5d385de9488daa154c3d5b05f225",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c10eb9ebdb54a8a9bf24c8e31be5465",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading weights file https://huggingface.co/allegro/herbert-large-cased/resolve/main/pytorch_model.bin from cache at /home/marek/.cache/huggingface/transformers/ca58839b8e4b1222703e13158ffeb3a5a7330260cbc39513f74710674d70268b.ad71128a5739887a02bfa6de2fa8768f86e02cd13d0c308873f4cdba254e4c7c\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "f1 = []\n",
    "\n",
    "j = 1\n",
    "\n",
    "for train_cv, test_cv in cv:\n",
    "    ds = DatasetDict()\n",
    "    ds['train'] = Dataset.from_pandas(df[df.index.isin(train_cv)])\n",
    "    ds['valid'] = Dataset.from_pandas(df[df.index.isin(test_cv)])\n",
    "    \n",
    "    \n",
    "    ds = ds.map(encode_batch, batched=True, batch_size=len(ds['train']))\n",
    "    ds.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "    \n",
    "    \n",
    "    model = BertAdapterModel.from_pretrained('allegro/herbert-large-cased', config=config).to('cuda')\n",
    "    \n",
    "    adapter_name='fake_news_'+str(j)\n",
    "    \n",
    "    model.add_adapter(adapter_name,config=adapter_config)\n",
    "    model.add_classification_head(\n",
    "        adapter_name,\n",
    "        id2label=id2label,\n",
    "        num_labels=num_labels\n",
    "      )\n",
    "\n",
    "    model.train_adapter(adapter_name)\n",
    "    \n",
    "    \n",
    "    trainer = AdapterTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=ds[\"train\"],\n",
    "        eval_dataset=ds[\"valid\"],\n",
    "        compute_metrics=compute_accuracy_f1score,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    \n",
    "    eval_out = trainer.evaluate()\n",
    "    \n",
    "    acc.append(eval_out['eval_accuracy'])\n",
    "    f1.append(eval_out['eval_f1score'])\n",
    "    \n",
    "    j+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f95d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = np.array(acc)\n",
    "f1 = np.array(f1)\n",
    "\n",
    "print(\n",
    "    'adapters',\n",
    "    f'Accuracy {acc.mean():.3f}+-{acc.std():.3f}',\n",
    "    f'F1 Score {f1.mean():.3f}+-{f1.std():.3f}',\n",
    "    f' {acc.mean():.3f}+-{acc.std():.3f} | {f1.mean():.3f}+-{f1.std():.3f}'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48717235",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ada1f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = cv_Kfold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8080360e",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = []\n",
    "f1 = []\n",
    "\n",
    "for train_cv, test_cv in cv:\n",
    "    ds = DatasetDict()\n",
    "    ds['train'] = Dataset.from_pandas(df[df.index.isin(train_cv)])\n",
    "    ds['valid'] = Dataset.from_pandas(df[df.index.isin(test_cv)])\n",
    "    \n",
    "    \n",
    "    ds = ds.map(encode_batch, batched=True, batch_size=len(ds['train']))\n",
    "    ds.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "    \n",
    "    \n",
    "    model = BertAdapterModel.from_pretrained('allegro/herbert-large-cased', config=config).to('cuda')\n",
    "    \n",
    "    \n",
    "    model.add_adapter(adapter_name,config=adapter_config)\n",
    "    model.add_classification_head(\n",
    "        adapter_name,\n",
    "        id2label=id2label,\n",
    "        num_labels=num_labels\n",
    "      )\n",
    "\n",
    "    model.train_adapter(adapter_name)\n",
    "    \n",
    "    \n",
    "    trainer = AdapterTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=ds[\"train\"],\n",
    "        eval_dataset=ds[\"valid\"],\n",
    "        compute_metrics=compute_accuracy_f1score,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    \n",
    "    eval_out = trainer.evaluate()\n",
    "    \n",
    "    acc.append(eval_out['eval_accuracy'])\n",
    "    f1.append(eval_out['eval_f1score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64737988",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = np.array(acc)\n",
    "f1 = np.array(f1)\n",
    "\n",
    "print(\n",
    "    'adapters',\n",
    "    f'Accuracy {acc.mean():.3f}+-{acc.std():.3f}',\n",
    "    f'F1 Score {f1.mean():.3f}+-{f1.std():.3f}',\n",
    "    f' {acc.mean():.3f}+-{acc.std():.3f} | {f1.mean():.3f}+-{f1.std():.3f}'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da063a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
