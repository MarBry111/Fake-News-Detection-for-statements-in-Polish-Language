{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "cb3587c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import gensim\n",
    "import nltk\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, \n",
    "    f1_score, \n",
    "    recall_score,\n",
    "    precision_score,\n",
    "    roc_auc_score, confusion_matrix, roc_curve, classification_report\n",
    ")\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.base import clone as sklearn_clone\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "import random\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from helpers.model import (\n",
    "    balance_data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "181a1d39-8608-42a4-bd81-cae872c7d214",
   "metadata": {},
   "outputs": [],
   "source": [
    "UNDERSAMPLING = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2148d9d1",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "8e578f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_styl = pd.read_parquet('../datasets/used_data/02_classical_ml/02_01_benchmark_styllometric_features.parquet')\n",
    "df_pos = pd.read_parquet('../datasets/used_data/02_classical_ml/02_02_benchmark_POS_ngrams.parquet')\n",
    "df_ngram = pd.read_parquet('../datasets/used_data/02_classical_ml/02_03_benchmark_words_ngrams.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "2a25689c-b8ad-4f22-a2cd-7582b90b060b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2394, 30) (2394, 1170) (2394, 1120)\n"
     ]
    }
   ],
   "source": [
    "print(df_styl.shape, df_pos.shape, df_ngram.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "f170b532-6dc2-4d45-bf76-b9d826eca059",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = df_styl['assestment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "2ae97f2f-5d81-4c7b-a388-b011d3400fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_styl.pop('assestment');\n",
    "df_pos.pop('assestment');\n",
    "df_ngram.pop('assestment');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed20c751",
   "metadata": {},
   "source": [
    "## Make balanced datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "eb5343ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_u, y_train_u = balance_data(df_styl, y_train, undersampling=UNDERSAMPLING, seed=111)\n",
    "X_pos_u, _ = balance_data(df_pos, y_train, undersampling=UNDERSAMPLING, seed=111)\n",
    "X_ngram_u, _ = balance_data(df_ngram, y_train, undersampling=UNDERSAMPLING, seed=111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "6e696f3d-a547-4694-aca5-4ebe6ae3fc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = X_ngram_u['TEXT_WORD'].str.split(' ').values\n",
    "X_ngram_u.pop('TEXT_WORD');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73c34bd",
   "metadata": {},
   "source": [
    "## CV creation\n",
    "### LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "a74ee952",
   "metadata": {},
   "outputs": [],
   "source": [
    "ideal_topic_num = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "a02f856f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1282/1282 [00:00<00:00, 9000.06it/s]\n"
     ]
    }
   ],
   "source": [
    "dictionary = gensim.corpora.Dictionary(words)\n",
    "\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in words]\n",
    "\n",
    "\n",
    "lda_model =  gensim.models.LdaMulticore(bow_corpus, \n",
    "                                   num_topics = ideal_topic_num, \n",
    "                                   id2word = dictionary,                                    \n",
    "                                   passes = 10,\n",
    "                                   random_state=111,\n",
    "                                   workers = 7)\n",
    "\n",
    "topics = []\n",
    "\n",
    "for line in tqdm(words):\n",
    "    line_bow = dictionary.doc2bow(line)\n",
    "    doc_lda = lda_model[line_bow]\n",
    "    \n",
    "    topics.append( max(doc_lda, key=lambda x:x[1])[0] )\n",
    "\n",
    "# X_train_u['topic'] = topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "fd65141c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.021*\"podatek\" + 0.018*\"energia\" + 0.011*\"rocznie\" + 0.011*\"możliwość\" + 0.010*\"mały\"\n",
      "1 0.029*\"procent\" + 0.017*\"sprawa\" + 0.012*\"móc\" + 0.012*\"minister\" + 0.012*\"mówić\"\n",
      "2 0.024*\"sprawiedliwość\" + 0.023*\"partia\" + 0.016*\"pis\" + 0.012*\"praca\" + 0.011*\"wybory\"\n",
      "3 0.025*\"budżet\" + 0.017*\"program\" + 0.016*\"sędzia\" + 0.016*\"państwo\" + 0.015*\"szkoła\"\n",
      "4 0.014*\"swój\" + 0.013*\"kaczyński\" + 0.011*\"prawo\" + 0.011*\"trybunał\" + 0.011*\"polak\"\n",
      "5 0.017*\"the\" + 0.015*\"przyp\" + 0.014*\"uchodźca\" + 0.013*\"badać\" + 0.011*\"czas\"\n",
      "6 0.048*\"europejski\" + 0.038*\"unia\" + 0.016*\"komisja\" + 0.013*\"rząd\" + 0.013*\"duży\"\n",
      "7 0.025*\"miasto\" + 0.019*\"liczba\" + 0.018*\"chodzić\" + 0.016*\"kraków\" + 0.013*\"rząd\"\n",
      "8 0.033*\"kraj\" + 0.023*\"europa\" + 0.021*\"polak\" + 0.021*\"człowiek\" + 0.020*\"platforma\"\n",
      "9 0.019*\"węgiel\" + 0.018*\"prezydent\" + 0.018*\"ukraina\" + 0.016*\"rosja\" + 0.012*\"europejski\"\n"
     ]
    }
   ],
   "source": [
    "x = lda_model.show_topics(num_topics=ideal_topic_num, num_words=5)\n",
    "\n",
    "for topic,word in x:\n",
    "    print(topic, word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "9351146f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>assestment</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>57</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>46</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>62</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>79</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>66</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>90</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>65</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>66</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "assestment   0   1\n",
       "topic             \n",
       "0           57  56\n",
       "1           46  56\n",
       "2           60  51\n",
       "3           50  63\n",
       "4           62  62\n",
       "5           79  59\n",
       "6           66  75\n",
       "7           90  85\n",
       "8           65  73\n",
       "9           66  61"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_u_topics = pd.DataFrame(y_train_u.copy())\n",
    "y_train_u_topics['topic'] = topics\n",
    "y_train_u_topics['n'] = 1\n",
    "y_train_u_topics.groupby(['topic', 'assestment']).sum().reset_index().pivot(index='topic',columns='assestment',values='n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e17e0a",
   "metadata": {},
   "source": [
    "### Kfold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "8829acdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_fold = []\n",
    "cv_fold_i = []\n",
    "\n",
    "for i in y_train_u_topics['topic'].unique().reshape(10,-1):\n",
    "    train_cv = X_train_u.index[ ~np.isin(y_train_u_topics[\"topic\"], i) ].values\n",
    "    test_cv = X_train_u.index[ np.isin(y_train_u_topics[\"topic\"], i) ].values\n",
    "    \n",
    "    # train_cv_i = X_train_u.reset_index().index[ ~np.isin(X_train_u[\"topic\"], i) ].values\n",
    "    # test_cv_i = X_train_u.reset_index().index[ np.isin(X_train_u[\"topic\"], i) ].values\n",
    "    \n",
    "    cv_fold.append( [train_cv, test_cv])\n",
    "    # cv_fold_i.append( [train_cv_i, test_cv_i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "bec90100",
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=10, shuffle=True)\n",
    "kf.get_n_splits(X_train_u)\n",
    "\n",
    "cv_Kfold = []\n",
    "cv_Kfold_i = []\n",
    "\n",
    "for train_index, test_index in kf.split(X_train_u):\n",
    "    train_cv = X_train_u.iloc[ train_index, : ].index.values\n",
    "    test_cv = X_train_u.iloc[ test_index, : ].index.values\n",
    "\n",
    "    # train_cv_i= X_train_u.reset_index().iloc[ train_index, : ].index.values\n",
    "    # test_cv_i = X_train_u.reset_index().iloc[ test_index, : ].index.values\n",
    "    \n",
    "    cv_Kfold.append( [train_cv, test_cv])\n",
    "    # cv_Kfold_i.append( [train_cv_i, test_cv_i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f0f426",
   "metadata": {},
   "source": [
    "## Run experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "4ff6e568",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(X, y, cv, clf_org, r_min=0.05):\n",
    "\n",
    "    results = {\n",
    "        'test_accuracy' : [],\n",
    "        'test_precision' : [],\n",
    "        'test_recall' : [],\n",
    "        'test_f1' : []\n",
    "    }\n",
    "\n",
    "    c_matrix = np.zeros((2,2))\n",
    "\n",
    "    for train_cv, test_cv in cv:\n",
    "        clf = sklearn_clone(clf_org)\n",
    "        \n",
    "        X_train_t = X[X.index.isin(train_cv)]\n",
    "        y_train_t = y[y.index.isin(train_cv)]\n",
    "\n",
    "        # keep only columns with corr > 0.05\n",
    "        if r_min:\n",
    "            col_keep = []\n",
    "            for c in X_train_t.columns:\n",
    "                min_v =X_train_t[c].values.min()\n",
    "                max_v = X_train_t[c].values.max()\n",
    "    \n",
    "                if min_v < max_v:\n",
    "                    r = scipy.stats.pearsonr(X_train_t[c].values, y_train_t)[0]\n",
    "                    if ~np.isnan(r) and r > r_min:\n",
    "                        col_keep.append(c)\n",
    "            \n",
    "            if len(col_keep) == 0:\n",
    "                print('No values returned')\n",
    "        \n",
    "            X_train_t = X_train_t[col_keep]\n",
    "        else:\n",
    "            col_keep =  X_train_t.columns.values.tolist()\n",
    "\n",
    "\n",
    "        X_test_t = X[X.index.isin(test_cv)]\n",
    "        y_test_t = y[y.index.isin(test_cv)]\n",
    "        \n",
    "        if r_min:\n",
    "            X_test_t = X_test_t[col_keep]\n",
    "\n",
    "        clf.fit(X_train_t, y_train_t)\n",
    "\n",
    "        y_pred = clf.predict(X_test_t)\n",
    "\n",
    "        confusion = confusion_matrix(y_test_t, y_pred)\n",
    "        c_matrix += confusion\n",
    "\n",
    "    #     TN, FP = confusion[0, 0], confusion[0, 1]\n",
    "    #     FN, TP = confusion[1, 0], confusion[1, 1]\n",
    "\n",
    "        results['test_accuracy'].append( accuracy_score(y_test_t, y_pred) ) \n",
    "        results['test_precision'].append( precision_score(y_test_t, y_pred) ) \n",
    "        results['test_recall'].append( recall_score(y_test_t, y_pred) ) \n",
    "        results['test_f1'].append( f1_score(y_test_t, y_pred) ) \n",
    "    \n",
    "    metrics = {\n",
    "        \"Accuracy\": np.array(results['test_accuracy']),\n",
    "        \"Precision\": np.array(results['test_precision']).mean(),\n",
    "        \"Recall\": np.array(results['test_recall']).mean(),\n",
    "        \"F1 Score\":  np.array(results['test_f1']),\n",
    "        \"Cols used\": col_keep\n",
    "        }\n",
    "\n",
    "#     print(c_matrix)\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87831898",
   "metadata": {},
   "source": [
    "## Topics Kfold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "dd76287f",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "0310f565-87d2-41a2-8d35-644c848cfa7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_lr_1 = LogisticRegression(max_iter=5000, C=1, penalty='l2', solver='liblinear')\n",
    "clf_lr_01 = LogisticRegression(max_iter=5000, C=0.1, penalty='l2', solver='liblinear')\n",
    "clf_rf = RandomForestClassifier(random_state=111, max_depth=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "923a0654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ngrams   lr C1 \n",
      "\tAccuracy 0.51+-0.04 \n",
      "\tF1 Score 0.38+-0.08\n",
      "features lr C1 \n",
      "\tAccuracy 0.51+-0.06 \n",
      "\tF1 Score 0.51+-0.06\n",
      "pos      lr C1 \n",
      "\tAccuracy 0.51+-0.04 \n",
      "\tF1 Score 0.39+-0.07\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for clf_used, clf_name in zip(\n",
    "    [\n",
    "        clf_lr_1, \n",
    "        # clf_rf, clf_xgb\n",
    "    ],[\n",
    "        'lr C1', \n",
    "        # 'rf d5', 'xgb  '\n",
    "    ]\n",
    "):\n",
    "        \n",
    "    for X_used, x_name, r_min in zip(\n",
    "        [X_ngram_u, X_train_u, X_pos_u],\n",
    "        ['ngrams  ', 'features', 'pos     '],\n",
    "        [0.05, None, 0.05]\n",
    "    ):\n",
    "        out = run_experiment(X_used, y_train_u, cv_fold, clf_used, r_min)\n",
    "        print(\n",
    "            x_name, \n",
    "            clf_name,\n",
    "            f'\\n\\tAccuracy {out[\"Accuracy\"].mean():.2f}+-{out[\"Accuracy\"].std():.2f}',\n",
    "            f'\\n\\tF1 Score {out[\"F1 Score\"].mean():.2f}+-{out[\"F1 Score\"].std():.2f}',\n",
    "            # f'Cols used {len(out[\"Cols used\"])}',\n",
    "            # f'\\n\\tPrecision {out[\"Precision\"].mean():.2f}+-{out[\"Precision\"].std():.2f}',\n",
    "            # f'\\n\\tRecall {out[\"Recall\"].mean():.2f}+-{out[\"Recall\"].std():.2f}',\n",
    "            # f' {out[\"Accuracy\"].mean():.3f}+-{out[\"Accuracy\"].std():.3f} | {out[\"F1 Score\"].mean():.3f}+-{out[\"F1 Score\"].std():.3f}'\n",
    "        )\n",
    "        \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed30078",
   "metadata": {},
   "source": [
    "## Random Kfold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "191ccf91-7937-44c5-bf2d-b7e30d422bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ngrams   lr C1 \n",
      "\tAccuracy 0.50+-0.05 \n",
      "\tF1 Score 0.38+-0.05\n",
      "features lr C1 \n",
      "\tAccuracy 0.51+-0.04 \n",
      "\tF1 Score 0.49+-0.04\n",
      "pos      lr C1 \n",
      "\tAccuracy 0.50+-0.04 \n",
      "\tF1 Score 0.39+-0.06\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for clf_used, clf_name in zip(\n",
    "    [\n",
    "        clf_lr_1, \n",
    "        # clf_rf, clf_xgb\n",
    "    ],[\n",
    "        'lr C1', \n",
    "        # 'rf d5', 'xgb  '\n",
    "    ]\n",
    "):\n",
    "        \n",
    "    for X_used, x_name, r_min in zip(\n",
    "        [X_ngram_u, X_train_u, X_pos_u],\n",
    "        ['ngrams  ', 'features', 'pos     '],\n",
    "        [0.05, 0, 0.05]\n",
    "    ):\n",
    "        out = run_experiment(X_used, y_train_u, cv_Kfold, clf_used, r_min)\n",
    "        print(\n",
    "            x_name, \n",
    "            clf_name,\n",
    "            f'\\n\\tAccuracy {out[\"Accuracy\"].mean():.2f}+-{out[\"Accuracy\"].std():.2f}',\n",
    "            f'\\n\\tF1 Score {out[\"F1 Score\"].mean():.2f}+-{out[\"F1 Score\"].std():.2f}',\n",
    "            # f'Cols used {len(out[\"Cols used\"])}',\n",
    "            # f'\\n\\tPrecision {out[\"Precision\"].mean():.2f}+-{out[\"Precision\"].std():.2f}',\n",
    "            # f'\\n\\tRecall {out[\"Recall\"].mean():.2f}+-{out[\"Recall\"].std():.2f}',\n",
    "            # f' {out[\"Accuracy\"].mean():.3f}+-{out[\"Accuracy\"].std():.3f} | {out[\"F1 Score\"].mean():.3f}+-{out[\"F1 Score\"].std():.3f}'\n",
    "        )\n",
    "        \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcb0bf7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
