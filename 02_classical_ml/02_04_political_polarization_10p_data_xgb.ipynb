{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb3587c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import gensim\n",
    "import nltk\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, \n",
    "    f1_score, \n",
    "    recall_score,\n",
    "    precision_score,\n",
    "    roc_auc_score, confusion_matrix, roc_curve, classification_report\n",
    ")\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.base import clone as sklearn_clone\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "import random\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from helpers.model import (\n",
    "    balance_data\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2148d9d1",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e578f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_styl = pd.read_parquet('../datasets/used_data/02_classical_ml/04_01_political_polarization_10p_styllometric_features.parquet')\n",
    "df_pos = pd.read_parquet('../datasets/used_data/02_classical_ml/04_02_political_polarization_10p_POS_ngrams.parquet')\n",
    "df_ngram = pd.read_parquet('../datasets/used_data/02_classical_ml/04_03_political_polarization_10p_words_ngrams.parquet')\n",
    "\n",
    "with open('../datasets/used_data/02_classical_ml/04_05_political_polarization_10p_herbert.npy', 'rb') as f:\n",
    "    df_herbert = pd.DataFrame(np.load(f))\n",
    "\n",
    "# with open('../datasets/used_data/02_classical_ml/04_05_political_polarization_10p_roberta.npy', 'rb') as f:\n",
    "#     df_roberta = pd.DataFrame(np.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a25689c-b8ad-4f22-a2cd-7582b90b060b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 28) (20000, 4999) (20000, 6537) (20000, 1024)\n"
     ]
    }
   ],
   "source": [
    "print(df_styl.shape, df_pos.shape, df_ngram.shape, df_herbert.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f170b532-6dc2-4d45-bf76-b9d826eca059",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = df_styl['assestment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a75d7c2-0507-4735-9353-1345233c5073",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "assestment\n",
       "0    12096\n",
       "1     7904\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ae97f2f-5d81-4c7b-a388-b011d3400fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_styl.pop('assestment');\n",
    "df_pos.pop('assestment');\n",
    "df_ngram.pop('assestment');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed20c751",
   "metadata": {},
   "source": [
    "## Make balanced datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb5343ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_u = df_styl\n",
    "X_pos_u = df_pos \n",
    "X_ngram_u = df_ngram\n",
    "X_herbert_u = df_herbert\n",
    "# X_roberta_u = df_roberta\n",
    "\n",
    "y_train_u = y_train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e696f3d-a547-4694-aca5-4ebe6ae3fc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = X_ngram_u['TEXT_WORD'].str.split(' ').values\n",
    "X_ngram_u.pop('TEXT_WORD');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73c34bd",
   "metadata": {},
   "source": [
    "## CV creation\n",
    "### Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6435e1cb-7062-4911-893e-f05e29353090",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../datasets/used_data/03_bert_like_models/03_topics_plytical_polarization.npy', 'rb') as f:\n",
    "    topics = np.load(f).tolist()\n",
    "\n",
    "y_train_u_topics = pd.DataFrame(y_train_u.copy())\n",
    "y_train_u_topics['topic'] = topics\n",
    "y_train_u_topics['n'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e17e0a",
   "metadata": {},
   "source": [
    "### Kfold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8829acdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_fold = []\n",
    "cv_fold_i = []\n",
    "\n",
    "for i in y_train_u_topics['topic'].unique().reshape(10,-1):\n",
    "    train_cv = X_train_u.index[ ~np.isin(y_train_u_topics[\"topic\"], i) ].values\n",
    "    test_cv = X_train_u.index[ np.isin(y_train_u_topics[\"topic\"], i) ].values\n",
    "    \n",
    "    # train_cv_i = X_train_u.reset_index().index[ ~np.isin(X_train_u[\"topic\"], i) ].values\n",
    "    # test_cv_i = X_train_u.reset_index().index[ np.isin(X_train_u[\"topic\"], i) ].values\n",
    "    \n",
    "    cv_fold.append( [train_cv, test_cv])\n",
    "    # cv_fold_i.append( [train_cv_i, test_cv_i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bec90100",
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=10, shuffle=True)\n",
    "kf.get_n_splits(X_train_u)\n",
    "\n",
    "cv_Kfold = []\n",
    "cv_Kfold_i = []\n",
    "\n",
    "for train_index, test_index in kf.split(X_train_u):\n",
    "    train_cv = X_train_u.iloc[ train_index, : ].index.values\n",
    "    test_cv = X_train_u.iloc[ test_index, : ].index.values\n",
    "\n",
    "    # train_cv_i= X_train_u.reset_index().iloc[ train_index, : ].index.values\n",
    "    # test_cv_i = X_train_u.reset_index().iloc[ test_index, : ].index.values\n",
    "    \n",
    "    cv_Kfold.append( [train_cv, test_cv])\n",
    "    # cv_Kfold_i.append( [train_cv_i, test_cv_i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f0f426",
   "metadata": {},
   "source": [
    "## Run experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4ff6e568",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(X, y, cv, clf_org, r_min=0.05):\n",
    "\n",
    "    results = {\n",
    "        'test_accuracy' : [],\n",
    "        'test_precision' : [],\n",
    "        'test_recall' : [],\n",
    "        'test_f1' : [],\n",
    "        'col_keep' : []\n",
    "    }\n",
    "\n",
    "    c_matrix = np.zeros((2,2))\n",
    "\n",
    "    for train_cv, test_cv in cv:\n",
    "        clf = sklearn_clone(clf_org)\n",
    "        \n",
    "        X_train_t = X[X.index.isin(train_cv)]\n",
    "        y_train_t = y[y.index.isin(train_cv)]\n",
    "\n",
    "        # keep only columns with corr > 0.05\n",
    "        if r_min:\n",
    "            col_keep = []\n",
    "            for c in X_train_t.columns:\n",
    "                min_v =X_train_t[c].values.min()\n",
    "                max_v = X_train_t[c].values.max()\n",
    "    \n",
    "                if min_v < max_v:\n",
    "                    r = scipy.stats.pearsonr(X_train_t[c].values, y_train_t)[0]\n",
    "                    if ~np.isnan(r) and r > r_min:\n",
    "                        col_keep.append(c)\n",
    "            \n",
    "            if len(col_keep) == 0:\n",
    "                print('No values returned')\n",
    "        \n",
    "            X_train_t = X_train_t[col_keep]\n",
    "        else:\n",
    "            col_keep =  X_train_t.columns.values.tolist()\n",
    "\n",
    "\n",
    "        X_test_t = X[X.index.isin(test_cv)]\n",
    "        y_test_t = y[y.index.isin(test_cv)]\n",
    "        \n",
    "        if r_min:\n",
    "            X_test_t = X_test_t[col_keep]\n",
    "\n",
    "        clf.fit(X_train_t, y_train_t)\n",
    "\n",
    "        y_pred = clf.predict(X_test_t)\n",
    "\n",
    "        confusion = confusion_matrix(y_test_t, y_pred)\n",
    "        c_matrix += confusion\n",
    "\n",
    "    #     TN, FP = confusion[0, 0], confusion[0, 1]\n",
    "    #     FN, TP = confusion[1, 0], confusion[1, 1]\n",
    "\n",
    "        results['test_accuracy'].append( accuracy_score(y_test_t, y_pred) ) \n",
    "        results['test_precision'].append( precision_score(y_test_t, y_pred) ) \n",
    "        results['test_recall'].append( recall_score(y_test_t, y_pred) ) \n",
    "        results['test_f1'].append( f1_score(y_test_t, y_pred) ) \n",
    "        results['col_keep'].append( len(col_keep)) \n",
    "    \n",
    "    metrics = {\n",
    "        \"Accuracy\": np.array(results['test_accuracy']),\n",
    "        \"Precision\": np.array(results['test_precision']).mean(),\n",
    "        \"Recall\": np.array(results['test_recall']).mean(),\n",
    "        \"F1 Score\":  np.array(results['test_f1']),\n",
    "        \"Cols used\": np.array(results['col_keep']),\n",
    "        }\n",
    "\n",
    "#     print(c_matrix)\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87831898",
   "metadata": {},
   "source": [
    "## Topics Kfold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dd76287f",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0310f565-87d2-41a2-8d35-644c848cfa7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_xgb = {\n",
    "    'n_estimators': 100,  # Number of boosting rounds\n",
    "    'learning_rate': 0.05,  # Step size shrinkage to prevent overfitting\n",
    "    'max_depth': 3,  # Maximum depth of a tree\n",
    "    'min_child_weight': 1,  # Minimum sum of instance weight needed in a child\n",
    "    'subsample': 0.9,  # Fraction of samples used for fitting the trees\n",
    "    'colsample_bytree': 0.9,  # Fraction of features used for fitting the trees\n",
    "    'gamma': 1,  # Minimum loss reduction required to make a further partition on a leaf node\n",
    "    'random_state': 111,  # Seed for reproducibility\n",
    "    'reg_alpha': 0.1,\n",
    "    'reg_lambda': 0.1,\n",
    "}\n",
    "\n",
    "clf_xgb = xgb.XGBClassifier(**params_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "923a0654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ngrams  None xgb   Accuracy 0.79+-0.15 F1 Score 0.76+-0.17 Cols used 6535+-0\n",
      "ngrams  0.03 xgb   Accuracy 0.75+-0.07 F1 Score 0.62+-0.14 Cols used 540+-63\n",
      "features     xgb   Accuracy 0.69+-0.04 F1 Score 0.62+-0.11 Cols used 27+-0\n",
      "pos     None xgb   Accuracy 0.61+-0.06 F1 Score 0.38+-0.09 Cols used 4998+-0\n",
      "pos     0.03 xgb   Accuracy 0.60+-0.06 F1 Score 0.28+-0.07 Cols used 96+-23\n",
      "herbert      xgb   Accuracy 0.90+-0.02 F1 Score 0.86+-0.07 Cols used 1024+-0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for clf_used, clf_name in zip(\n",
    "    [\n",
    "        clf_xgb\n",
    "    ],[\n",
    "        'xgb  '\n",
    "    ]\n",
    "):\n",
    "        \n",
    "    for X_used, x_name, r_min in zip(\n",
    "         [X_ngram_u, \n",
    "         X_ngram_u, \n",
    "         # X_ngram_u, \n",
    "         # X_ngram_u, \n",
    "         X_train_u, \n",
    "         X_pos_u, \n",
    "         X_pos_u, \n",
    "         # X_pos_u, \n",
    "         # X_pos_u, \n",
    "         X_herbert_u],\n",
    "        ['ngrams  None', \n",
    "         # 'ngrams  0.01', \n",
    "         'ngrams  0.03', \n",
    "         # 'ngrams  0.05', \n",
    "         'features    ', \n",
    "         'pos     None', \n",
    "         # 'pos     0.01', \n",
    "         'pos     0.03', \n",
    "         # 'pos     0.05', \n",
    "         'herbert     '],\n",
    "        [None, \n",
    "         # 0.01, \n",
    "         0.03,\n",
    "         # 0.05, \n",
    "         None, \n",
    "         None, \n",
    "         # 0.01, \n",
    "         0.03, \n",
    "         # 0.05, \n",
    "         None]\n",
    "    ):\n",
    "        out = run_experiment(X_used, y_train_u, cv_fold, clf_used, r_min)\n",
    "        print(\n",
    "            x_name, \n",
    "            clf_name,\n",
    "            f'Accuracy {out[\"Accuracy\"].mean():.2f}+-{out[\"Accuracy\"].std():.2f}',\n",
    "            f'F1 Score {out[\"F1 Score\"].mean():.2f}+-{out[\"F1 Score\"].std():.2f}',\n",
    "            f'Cols used {out[\"Cols used\"].mean().round(0):.0f}+-{out[\"Cols used\"].std().round(0):.0f}',\n",
    "            # f'Cols used {len(out[\"Cols used\"])}',\n",
    "            # f'\\n\\tPrecision {out[\"Precision\"].mean():.2f}+-{out[\"Precision\"].std():.2f}',\n",
    "            # f'\\n\\tRecall {out[\"Recall\"].mean():.2f}+-{out[\"Recall\"].std():.2f}',\n",
    "            # f' {out[\"Accuracy\"].mean():.3f}+-{out[\"Accuracy\"].std():.3f} | {out[\"F1 Score\"].mean():.3f}+-{out[\"F1 Score\"].std():.3f}'\n",
    "        )\n",
    "        \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed30078",
   "metadata": {},
   "source": [
    "## Random Kfold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "191ccf91-7937-44c5-bf2d-b7e30d422bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ngrams  None xgb   Accuracy 0.86+-0.01 F1 Score 0.83+-0.01 Cols used 6535+-0\n",
      "ngrams  0.03 xgb   Accuracy 0.79+-0.01 F1 Score 0.68+-0.01 Cols used 532+-12\n",
      "features     xgb   Accuracy 0.72+-0.01 F1 Score 0.68+-0.02 Cols used 27+-0\n",
      "pos     None xgb   Accuracy 0.67+-0.01 F1 Score 0.43+-0.02 Cols used 4998+-0\n",
      "pos     0.03 xgb   Accuracy 0.65+-0.01 F1 Score 0.31+-0.02 Cols used 82+-3\n",
      "herbert      xgb   Accuracy 0.92+-0.01 F1 Score 0.90+-0.01 Cols used 1024+-0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for clf_used, clf_name in zip(\n",
    "    [\n",
    "        clf_xgb\n",
    "    ],[\n",
    "        'xgb  '\n",
    "    ]\n",
    "):\n",
    "        \n",
    "    for X_used, x_name, r_min in zip(\n",
    "        [X_ngram_u, \n",
    "         X_ngram_u, \n",
    "         # X_ngram_u, \n",
    "         # X_ngram_u, \n",
    "         X_train_u, \n",
    "         X_pos_u, \n",
    "         X_pos_u, \n",
    "         # X_pos_u, \n",
    "         # X_pos_u, \n",
    "         X_herbert_u],\n",
    "        ['ngrams  None', \n",
    "         # 'ngrams  0.01', \n",
    "         'ngrams  0.03', \n",
    "         # 'ngrams  0.05', \n",
    "         'features    ', \n",
    "         'pos     None', \n",
    "         # 'pos     0.01', \n",
    "         'pos     0.03', \n",
    "         # 'pos     0.05', \n",
    "         'herbert     '],\n",
    "        [None, \n",
    "         # 0.01, \n",
    "         0.03,\n",
    "         # 0.05, \n",
    "         None, \n",
    "         None, \n",
    "         # 0.01, \n",
    "         0.03, \n",
    "         # 0.05, \n",
    "         None]\n",
    "    ):\n",
    "        out = run_experiment(X_used, y_train_u, cv_Kfold, clf_used, r_min)\n",
    "        print(\n",
    "            x_name, \n",
    "            clf_name,\n",
    "            f'Accuracy {out[\"Accuracy\"].mean():.2f}+-{out[\"Accuracy\"].std():.2f}',\n",
    "            f'F1 Score {out[\"F1 Score\"].mean():.2f}+-{out[\"F1 Score\"].std():.2f}',\n",
    "            f'Cols used {out[\"Cols used\"].mean().round(0):.0f}+-{out[\"Cols used\"].std().round(0):.0f}',\n",
    "            # f'Cols used {len(out[\"Cols used\"])}',\n",
    "            # f'\\n\\tPrecision {out[\"Precision\"].mean():.2f}+-{out[\"Precision\"].std():.2f}',\n",
    "            # f'\\n\\tRecall {out[\"Recall\"].mean():.2f}+-{out[\"Recall\"].std():.2f}',\n",
    "            # f' {out[\"Accuracy\"].mean():.3f}+-{out[\"Accuracy\"].std():.3f} | {out[\"F1 Score\"].mean():.3f}+-{out[\"F1 Score\"].std():.3f}'\n",
    "        )\n",
    "        \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcb0bf7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
