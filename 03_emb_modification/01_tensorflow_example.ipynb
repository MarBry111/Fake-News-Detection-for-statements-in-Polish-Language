{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fe6cf5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# based on this post\n",
    "# https://medium.com/swlh/few-shot-learning-in-nlp-use-siamese-networks-189de22459d0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "553f76cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3738908f",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Op type not registered 'SentencepieceOp' in binary running on marek-ROG-Zephyrus-G14. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing (e.g.) `tf.contrib.resampler` should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed.\n You may be trying to load on a different device from the computational device. Consider setting the `experimental_io_device` option in `tf.saved_model.LoadOptions` to the io_device such as '/job:localhost'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:4098\u001b[0m, in \u001b[0;36mGraph._get_op_def\u001b[0;34m(self, type)\u001b[0m\n\u001b[1;32m   4097\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 4098\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_op_def_cache\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m   4099\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "\u001b[0;31mKeyError\u001b[0m: 'SentencepieceOp'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.9/site-packages/tensorflow/python/saved_model/load.py:938\u001b[0m, in \u001b[0;36mload_internal\u001b[0;34m(export_dir, tags, options, loader_cls, filters)\u001b[0m\n\u001b[1;32m    937\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 938\u001b[0m   loader \u001b[38;5;241m=\u001b[39m \u001b[43mloader_cls\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobject_graph_proto\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msaved_model_proto\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexport_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    939\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mckpt_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    940\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mNotFoundError \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.9/site-packages/tensorflow/python/saved_model/load.py:138\u001b[0m, in \u001b[0;36mLoader.__init__\u001b[0;34m(self, object_graph_proto, saved_model_proto, export_dir, ckpt_options, save_options, filters)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_export_dir \u001b[38;5;241m=\u001b[39m export_dir\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_concrete_functions \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 138\u001b[0m     \u001b[43mfunction_deserialization\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_function_def_library\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmeta_graph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgraph_def\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlibrary\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwrapper_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_WrapperFunction\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_options \u001b[38;5;241m=\u001b[39m ckpt_options\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.9/site-packages/tensorflow/python/saved_model/function_deserialization.py:388\u001b[0m, in \u001b[0;36mload_function_def_library\u001b[0;34m(library, load_shared_name_suffix, wrapper_function)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m graph\u001b[38;5;241m.\u001b[39mas_default():\n\u001b[0;32m--> 388\u001b[0m   func_graph \u001b[38;5;241m=\u001b[39m \u001b[43mfunction_def_lib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_def_to_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;66;03m# Restores gradients for function-call ops (not the same as ops that use\u001b[39;00m\n\u001b[1;32m    390\u001b[0m \u001b[38;5;66;03m# custom gradients)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.9/site-packages/tensorflow/python/framework/function_def_to_graph.py:63\u001b[0m, in \u001b[0;36mfunction_def_to_graph\u001b[0;34m(fdef, input_shapes)\u001b[0m\n\u001b[1;32m     62\u001b[0m     input_shapes \u001b[38;5;241m=\u001b[39m input_shapes_attr\u001b[38;5;241m.\u001b[39mlist\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m---> 63\u001b[0m graph_def, nested_to_flat_tensor_name \u001b[38;5;241m=\u001b[39m \u001b[43mfunction_def_to_graph_def\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfdef\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_shapes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m func_graph\u001b[38;5;241m.\u001b[39mas_default():\n\u001b[1;32m     67\u001b[0m   \u001b[38;5;66;03m# Add all function nodes to the graph.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.9/site-packages/tensorflow/python/framework/function_def_to_graph.py:229\u001b[0m, in \u001b[0;36mfunction_def_to_graph_def\u001b[0;34m(fdef, input_shapes)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 229\u001b[0m   op_def \u001b[38;5;241m=\u001b[39m \u001b[43mdefault_graph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_op_def\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode_def\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m attr \u001b[38;5;129;01min\u001b[39;00m op_def\u001b[38;5;241m.\u001b[39mattr:\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:4102\u001b[0m, in \u001b[0;36mGraph._get_op_def\u001b[0;34m(self, type)\u001b[0m\n\u001b[1;32m   4100\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m c_api_util\u001b[38;5;241m.\u001b[39mtf_buffer() \u001b[38;5;28;01mas\u001b[39;00m buf:\n\u001b[1;32m   4101\u001b[0m   \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m-> 4102\u001b[0m   \u001b[43mpywrap_tf_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTF_GraphGetOpDef\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_c_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4103\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mbuf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4104\u001b[0m   \u001b[38;5;66;03m# pylint: enable=protected-access\u001b[39;00m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: Op type not registered 'SentencepieceOp' in binary running on marek-ROG-Zephyrus-G14. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing (e.g.) `tf.contrib.resampler` should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m module_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://tfhub.dev/google/universal-sentence-encoder-multilingual-large/3\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Import the Universal Sentence Encoder's TF Hub module\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m embed \u001b[38;5;241m=\u001b[39m \u001b[43mhub\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhttps://tfhub.dev/google/universal-sentence-encoder-multilingual-large/3\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.9/site-packages/tensorflow_hub/module_v2.py:106\u001b[0m, in \u001b[0;36mload\u001b[0;34m(handle, tags, options)\u001b[0m\n\u001b[1;32m    103\u001b[0m   obj \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mcompat\u001b[38;5;241m.\u001b[39mv1\u001b[38;5;241m.\u001b[39msaved_model\u001b[38;5;241m.\u001b[39mload_v2(\n\u001b[1;32m    104\u001b[0m       module_path, tags\u001b[38;5;241m=\u001b[39mtags, options\u001b[38;5;241m=\u001b[39moptions)\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 106\u001b[0m   obj \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mv1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msaved_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_v2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtags\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m obj\u001b[38;5;241m.\u001b[39m_is_hub_module_v1 \u001b[38;5;241m=\u001b[39m is_hub_module_v1  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.9/site-packages/tensorflow/python/saved_model/load.py:900\u001b[0m, in \u001b[0;36mload\u001b[0;34m(export_dir, tags, options)\u001b[0m\n\u001b[1;32m    809\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msaved_model.load\u001b[39m\u001b[38;5;124m\"\u001b[39m, v1\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msaved_model.load_v2\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    810\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(export_dir, tags\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    811\u001b[0m   \u001b[38;5;124;03m\"\"\"Load a SavedModel from `export_dir`.\u001b[39;00m\n\u001b[1;32m    812\u001b[0m \n\u001b[1;32m    813\u001b[0m \u001b[38;5;124;03m  Signatures associated with the SavedModel are available as functions:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[38;5;124;03m    ValueError: If `tags` don't match a MetaGraph in the SavedModel.\u001b[39;00m\n\u001b[1;32m    899\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 900\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[43mload_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexport_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mroot\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    901\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.9/site-packages/tensorflow/python/saved_model/load.py:941\u001b[0m, in \u001b[0;36mload_internal\u001b[0;34m(export_dir, tags, options, loader_cls, filters)\u001b[0m\n\u001b[1;32m    938\u001b[0m   loader \u001b[38;5;241m=\u001b[39m loader_cls(object_graph_proto, saved_model_proto, export_dir,\n\u001b[1;32m    939\u001b[0m                       ckpt_options, options, filters)\n\u001b[1;32m    940\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mNotFoundError \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m--> 941\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[1;32m    942\u001b[0m       \u001b[38;5;28mstr\u001b[39m(err) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m You may be trying to load on a different device \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    943\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom the computational device. Consider setting the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    944\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`experimental_io_device` option in `tf.saved_model.LoadOptions` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    945\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto the io_device such as \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/job:localhost\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    946\u001b[0m root \u001b[38;5;241m=\u001b[39m loader\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    947\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(loader, Loader):\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: Op type not registered 'SentencepieceOp' in binary running on marek-ROG-Zephyrus-G14. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing (e.g.) `tf.contrib.resampler` should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed.\n You may be trying to load on a different device from the computational device. Consider setting the `experimental_io_device` option in `tf.saved_model.LoadOptions` to the io_device such as '/job:localhost'."
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "localhost_save_option = tf.saved_model.SaveOptions(experimental_io_device=\"/job:localhost\")\n",
    "\n",
    "module_url = 'https://tfhub.dev/google/universal-sentence-encoder-multilingual-large/3'\n",
    "# Import the Universal Sentence Encoder's TF Hub module\n",
    "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder-multilingual-large/3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b090b8b7",
   "metadata": {},
   "source": [
    "## Load train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2df31209",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../datasets/ready2use/fake_news_features_combined.csv', sep=';')\n",
    "\n",
    "df = df[ df['assestment'] != 'brak' ]\n",
    "\n",
    "df.loc[:, 'assestment'] = df['assestment'].replace({\n",
    "    'falsz' : 'Fałsz',\n",
    "    'zbity_zegar' : 'Fałsz',\n",
    "    'raczej_falsz' : 'Fałsz',\n",
    "    'prawda' : 'Prawda',\n",
    "    'blisko_prawdy' : 'Prawda',\n",
    "    'polprawda' : 'Manipulacja',\n",
    "    'Częściowy fałsz' : 'Manipulacja'\n",
    "})\n",
    "\n",
    "df = df[ df['assestment'] != 'Nieweryfikowalne' ]\n",
    "df = df[ df['assestment'] != 'Manipulacja' ]\n",
    "\n",
    "df['assestment'] = df['assestment'].replace({\n",
    "    'Fałsz' : 0,\n",
    "#     'Manipulacja' : 1,\n",
    "    'Prawda' : 1\n",
    "}).astype(int)\n",
    "\n",
    "y_train = df.copy()['assestment']\n",
    "X_train = df.copy().loc[:, df.columns != 'assestment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc7094fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = X_train.join(y_train)\n",
    "train = train[['text_clean','assestment']]\n",
    "train.columns = ['text','class']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd87d534",
   "metadata": {},
   "source": [
    "### Add random Kfold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "325db62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(train, test_size=0.1, random_state=111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac256ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import keras.backend as K\n",
    "from keras.layers import *\n",
    "from keras.callbacks import *\n",
    "from keras.optimizers import *\n",
    "from keras import Model\n",
    "from keras.layers.core import Lambda, Flatten, Dense\n",
    "from keras.layers import Bidirectional, LSTM\n",
    "import pickle    \n",
    "import os\n",
    "\n",
    "input_text1 = Input(shape=(512,))\n",
    "\n",
    "x = Dense(256, activation='relu')(input_text1)\n",
    "x = Dropout(0.4)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dense(64, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001))(x)\n",
    "x = Dropout(0.4)(x)\n",
    "\n",
    "dense_layer = Dense(128, name='dense_layer')(x)\n",
    "norm_layer = Lambda(lambda  x: K.l2_normalize(x, axis=1), name='norm_layer')(dense_layer)\n",
    "\n",
    "model=Model(inputs=[input_text1], outputs=norm_layer)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb32370",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input for anchor, positive and negative images\n",
    "in_a = Input(shape=(512,))\n",
    "in_p = Input(shape=(512,))\n",
    "in_n = Input(shape=(512,))\n",
    "\n",
    "# Output for anchor, positive and negative embedding vectors\n",
    "# The nn4_small model instance is shared (Siamese network)\n",
    "emb_a = model(in_a)\n",
    "emb_p = model(in_p)\n",
    "emb_n = model(in_n)\n",
    "\n",
    "class TripletLossLayer(Layer):\n",
    "    def __init__(self, alpha, **kwargs):\n",
    "        self.alpha = alpha\n",
    "        super(TripletLossLayer, self).__init__(**kwargs)\n",
    "    \n",
    "    def triplet_loss(self, inputs):\n",
    "        a, p, n = inputs\n",
    "        p_dist = K.sum(K.square(a-p), axis=-1)\n",
    "        n_dist = K.sum(K.square(a-n), axis=-1)\n",
    "        return K.sum(K.maximum(p_dist - n_dist + self.alpha, 0), axis=0)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        loss = self.triplet_loss(inputs)\n",
    "        self.add_loss(loss)\n",
    "        return loss\n",
    "\n",
    "# Layer that computes the triplet loss from anchor, positive and negative embedding vectors\n",
    "triplet_loss_layer = TripletLossLayer(alpha=0.4, name='triplet_loss_layer')([emb_a, emb_p, emb_n])\n",
    "\n",
    "# Model that can be trained with anchor, positive negative images\n",
    "nn4_small2_train = Model([in_a, in_p, in_n], triplet_loss_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3c0aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the necessary datastructures for selcting triplets\n",
    "unique_train_label=np.array(train['class'].unique().tolist())\n",
    "labels_train=np.array(train['class'].tolist())\n",
    "map_train_label_indices = {label: np.flatnonzero(labels_train == label) for label in unique_train_label}\n",
    "\n",
    "def get_triplets(unique_train_label,map_train_label_indices):\n",
    "    label_l, label_r = np.random.choice(unique_train_label, 2, replace=False)\n",
    "    a, p = np.random.choice(map_train_label_indices[label_l],2, replace=False)\n",
    "    n = np.random.choice(map_train_label_indices[label_r])\n",
    "    return a, p, n\n",
    "\n",
    "def get_triplets_batch(k,train_set,unique_train_label,map_train_label_indices,embed):\n",
    "\n",
    "    while True:\n",
    "        idxs_a, idxs_p, idxs_n = [], [], []\n",
    "        for _ in range(k):\n",
    "            a, p, n = get_triplets(unique_train_label,map_train_label_indices)\n",
    "            idxs_a.append(a)\n",
    "            idxs_p.append(p)\n",
    "            idxs_n.append(n)\n",
    "\n",
    "        a=train_set.iloc[idxs_a].values.tolist()\n",
    "        b=train_set.iloc[idxs_p].values.tolist()\n",
    "        c=train_set.iloc[idxs_n].values.tolist()\n",
    "\n",
    "        a = embed(a)\n",
    "        p = embed(b)\n",
    "        n = embed(c)\n",
    "        # return train_set[idxs_a], train_set[idxs_p], train_set[idxs_n]\n",
    "        yield [a,p,n], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77291700",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn4_small2_train.compile(loss=None, optimizer='adam')\n",
    "nn4_small2_train.fit(get_triplets_batch(128,train['text'],unique_train_label,map_train_label_indices,embed), epochs=100,steps_per_epoch=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b13227",
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the embeddings from the model\n",
    "X_train = model.predict(embed(np.array(train['text'].values.tolist())))\n",
    "X_test = model.predict(embed(np.array(test['text'].values.tolist())))\n",
    "\n",
    "y_train = np.array(train['class'].values.tolist())\n",
    "y_test = np.array(test['class'].values.tolist())\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=5, metric='euclidean')\n",
    "svc = LinearSVC()\n",
    "\n",
    "knn.fit(X_train, y_train)\n",
    "svc.fit(X_train, y_train)\n",
    "\n",
    "y_pred_knn = knn.predict(X_test)\n",
    "acc_knn = accuracy_score(y_test, y_pred_knn)\n",
    "y_pred_svc = svc.predict(X_test)\n",
    "acc_svc = accuracy_score(y_test, y_pred_svc)\n",
    "\n",
    "print(f'KNN accuracy = {acc_knn}, SVM accuracy = {acc_svc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512579dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.classification_report(list(y_test), list(y_pred_knn)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae125e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "X_embedded = TSNE(n_components=2).fit_transform(X_test)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "for i, t in enumerate(set(y_test)):\n",
    "    idx = y_test == t\n",
    "    plt.scatter(X_embedded[idx, 0], X_embedded[idx, 1], label=t)   \n",
    "\n",
    "plt.legend(bbox_to_anchor=(1, 1));"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
