{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-27T15:08:17.772784Z",
     "iopub.status.busy": "2022-11-27T15:08:17.772405Z",
     "iopub.status.idle": "2022-11-27T15:08:43.369468Z",
     "shell.execute_reply": "2022-11-27T15:08:43.368328Z",
     "shell.execute_reply.started": "2022-11-27T15:08:17.772745Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<stylo_metrix.pipeline.stylo_metrix_pipe.StyloMetrixPipe at 0x7fc20ed158e0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import time\n",
    "# import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# from tqdm.notebook import tqdm\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.base import clone as sklearn_clone\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, \n",
    "    f1_score, \n",
    "    recall_score,\n",
    "    precision_score,\n",
    "    roc_auc_score, confusion_matrix, roc_curve, classification_report\n",
    ")\n",
    "\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "# from transformers import AutoTokenizer, AutoModel, HerbertTokenizer, BatchEncoding\n",
    "\n",
    "# import gc\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# import scipy\n",
    "# from scipy import spatial\n",
    "\n",
    "# import umap.umap_ as umap\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import spacy\n",
    "import stylo_metrix\n",
    "\n",
    "nlp = spacy.load('pl_nask_large')         # for Polish\n",
    "nlp.add_pipe(\"stylo_metrix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-27T15:08:43.373442Z",
     "iopub.status.busy": "2022-11-27T15:08:43.372540Z",
     "iopub.status.idle": "2022-11-27T15:08:43.390384Z",
     "shell.execute_reply": "2022-11-27T15:08:43.388887Z",
     "shell.execute_reply.started": "2022-11-27T15:08:43.373390Z"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(111)\n",
    "random.seed(111)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-27T15:08:43.393489Z",
     "iopub.status.busy": "2022-11-27T15:08:43.392592Z",
     "iopub.status.idle": "2022-11-27T15:08:43.451933Z",
     "shell.execute_reply": "2022-11-27T15:08:43.450432Z",
     "shell.execute_reply.started": "2022-11-27T15:08:43.393432Z"
    }
   },
   "outputs": [],
   "source": [
    "df_topics = pd.read_csv('../datasets/ready2use/topics.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-27T15:08:43.456831Z",
     "iopub.status.busy": "2022-11-27T15:08:43.456352Z",
     "iopub.status.idle": "2022-11-27T15:08:43.636702Z",
     "shell.execute_reply": "2022-11-27T15:08:43.635861Z",
     "shell.execute_reply.started": "2022-11-27T15:08:43.456788Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../datasets/ready2use/fake_news_features_combined.csv', sep=';')\n",
    "\n",
    "df = df[ df['assestment'] != 'brak' ]\n",
    "\n",
    "df.loc[:, 'assestment'] = df['assestment'].replace({\n",
    "    'falsz' : 'Fałsz',\n",
    "    'zbity_zegar' : 'Fałsz',\n",
    "    'raczej_falsz' : 'Fałsz',\n",
    "    'prawda' : 'Prawda',\n",
    "    'blisko_prawdy' : 'Prawda',\n",
    "    'polprawda' : 'Manipulacja',\n",
    "    'Częściowy fałsz' : 'Manipulacja'\n",
    "})\n",
    "\n",
    "df = df[ df['assestment'] != 'Nieweryfikowalne' ]\n",
    "df = df[ df['assestment'] != 'Manipulacja' ]\n",
    "\n",
    "df['assestment'] = df['assestment'].replace({\n",
    "    'Fałsz' : 0,\n",
    "#     'Manipulacja' : 1,\n",
    "    'Prawda' : 1\n",
    "}).astype(int)\n",
    "\n",
    "df = df.copy()[['assestment', 'text_clean']][df.index.isin(df_topics.index)].reset_index(drop=True)\n",
    "\n",
    "embeddings_table = pd.read_csv('../datasets/ready2use/embeddings_pl_herbert.csv', sep=\",\", header=None).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kfold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-27T15:08:47.594715Z",
     "iopub.status.busy": "2022-11-27T15:08:47.594342Z",
     "iopub.status.idle": "2022-11-27T15:08:47.632847Z",
     "shell.execute_reply": "2022-11-27T15:08:47.631805Z",
     "shell.execute_reply.started": "2022-11-27T15:08:47.594681Z"
    }
   },
   "outputs": [],
   "source": [
    "cv_fold = []\n",
    "cv_fold_i = []\n",
    "\n",
    "for i in df_topics['topic'].unique().reshape(10,-1):\n",
    "    train_cv = df_topics.index[ ~np.isin(df_topics[\"topic\"], [i, np.mod(i+1,10)]) ].values\n",
    "    val_cv = df_topics.index[ np.isin(df_topics[\"topic\"], np.mod(i+1,10)) ].values\n",
    "    test_cv = df_topics.index[ np.isin(df_topics[\"topic\"], i) ].values\n",
    "    \n",
    "    train_cv_i = df_topics.reset_index().index[ ~np.isin(df_topics[\"topic\"], [i, np.mod(i+1,10)]) ].values\n",
    "    val_cv_i = df_topics.reset_index().index[ np.isin(df_topics[\"topic\"], np.mod(i+1,10)) ].values\n",
    "    test_cv_i = df_topics.reset_index().index[ np.isin(df_topics[\"topic\"], i) ].values\n",
    "    \n",
    "    cv_fold.append( [train_cv, val_cv, test_cv])\n",
    "    cv_fold_i.append( [train_cv_i, val_cv_i, test_cv_i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-27T15:08:47.635075Z",
     "iopub.status.busy": "2022-11-27T15:08:47.634315Z",
     "iopub.status.idle": "2022-11-27T15:08:47.687480Z",
     "shell.execute_reply": "2022-11-27T15:08:47.686035Z",
     "shell.execute_reply.started": "2022-11-27T15:08:47.635025Z"
    }
   },
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=10, shuffle=True)\n",
    "kf.get_n_splits(df_topics)\n",
    "\n",
    "cv_Kfold = []\n",
    "cv_Kfold_i = []\n",
    "\n",
    "for train_index, test_index in kf.split(df_topics):\n",
    "    train_index, val_index = train_test_split(train_index, test_size=1/9, shuffle=True)\n",
    "    train_cv = df_topics.iloc[ train_index, : ].index.values\n",
    "    val_cv = df_topics.iloc[ val_index, : ].index.values\n",
    "    test_cv = df_topics.iloc[ test_index, : ].index.values\n",
    "\n",
    "    train_cv_i= df_topics.reset_index().iloc[ train_index, : ].index.values\n",
    "    val_cv_i = df_topics.reset_index().iloc[ val_index, : ].index.values\n",
    "    test_cv_i = df_topics.reset_index().iloc[ test_index, : ].index.values\n",
    "    \n",
    "    cv_Kfold.append( [train_cv, val_cv, test_cv])\n",
    "    cv_Kfold_i.append( [train_cv_i, val_cv_i, test_cv_i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vec_stylo(txt):\n",
    "    doc = nlp(txt)\n",
    "    vec = []\n",
    "    for metric in doc._.smv:\n",
    "        vec.append(metric['value'])\n",
    "    return vec\n",
    "\n",
    "if False:\n",
    "    txt = df['text_clean'].values[0]\n",
    "    \n",
    "    emb_style = [get_vec_stylo(t) for t in tqdm(df['text_clean'].values, position=0, leave=True)]\n",
    "    \n",
    "    with open('../datasets/ready2use/style_emb_pl.npy', 'wb') as f:\n",
    "        np.save(f, np.array(emb_style) )\n",
    "        \n",
    "else:\n",
    "    with open('../datasets/ready2use/style_emb_pl.npy', 'rb') as f:\n",
    "        emb_style = np.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6541, 89)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(emb_style).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-26T15:53:33.387287Z",
     "iopub.status.busy": "2022-11-26T15:53:33.386511Z",
     "iopub.status.idle": "2022-11-26T15:53:33.402885Z",
     "shell.execute_reply": "2022-11-26T15:53:33.401582Z",
     "shell.execute_reply.started": "2022-11-26T15:53:33.387245Z"
    }
   },
   "outputs": [],
   "source": [
    "clf_lr_1 = LogisticRegression(max_iter=5000, C=1, penalty='l2', solver='liblinear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_index, val_index, test_index = cv_fold_i[np.random.randint(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6541, 1113)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_combined = np.concatenate([embeddings_table, emb_style],1)\n",
    "emb_combined.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-26T15:53:34.027251Z",
     "iopub.status.busy": "2022-11-26T15:53:34.026899Z",
     "iopub.status.idle": "2022-11-26T15:53:34.163303Z",
     "shell.execute_reply": "2022-11-26T15:53:34.161895Z",
     "shell.execute_reply.started": "2022-11-26T15:53:34.027223Z"
    }
   },
   "outputs": [],
   "source": [
    "results = {\n",
    "    'test_accuracy' : [],\n",
    "    'test_precision' : [],\n",
    "    'test_recall' : [],\n",
    "    'test_f1' : []\n",
    "}\n",
    "\n",
    "\n",
    "y_train_t = df['assestment'].values[train_index]\n",
    "X_train_t = emb_combined[train_index,:]\n",
    "y_test_t = df['assestment'].values[test_index]\n",
    "X_test_t = emb_combined[test_index,:]\n",
    "\n",
    "clf_lr_1.fit(X_train_t, y_train_t)\n",
    "\n",
    "y_pred = clf_lr_1.predict(X_test_t)\n",
    "\n",
    "results['test_accuracy'].append( accuracy_score(y_test_t, y_pred) ) \n",
    "results['test_precision'].append( precision_score(y_test_t, y_pred) ) \n",
    "results['test_recall'].append( recall_score(y_test_t, y_pred) ) \n",
    "results['test_f1'].append( f1_score(y_test_t, y_pred) ) \n",
    "\n",
    "metrics = {\n",
    "    \"Accuracy\": np.array(results['test_accuracy']),\n",
    "#     \"Precision\": np.array(results['test_precision']).mean(),\n",
    "#     \"Recall\": np.array(results['test_recall']).mean(),\n",
    "    \"F1 Score\":  np.array(results['test_f1']),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-26T15:53:34.788113Z",
     "iopub.status.busy": "2022-11-26T15:53:34.787753Z",
     "iopub.status.idle": "2022-11-26T15:53:34.799721Z",
     "shell.execute_reply": "2022-11-26T15:53:34.798092Z",
     "shell.execute_reply.started": "2022-11-26T15:53:34.788083Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[289, 109],\n",
       "       [104, 203]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test_t, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-26T15:53:36.407665Z",
     "iopub.status.busy": "2022-11-26T15:53:36.406494Z",
     "iopub.status.idle": "2022-11-26T15:53:36.415616Z",
     "shell.execute_reply": "2022-11-26T15:53:36.414441Z",
     "shell.execute_reply.started": "2022-11-26T15:53:36.407618Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Accuracy': array([0.69787234]), 'F1 Score': array([0.65589661])}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6541, 1113)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_combined.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test using CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-25T18:28:29.816271Z",
     "iopub.status.busy": "2022-11-25T18:28:29.815893Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 10/10 [00:19<00:00,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "triplet loss lr C1 Accuracy 0.698+-0.022 F1 Score 0.678+-0.033  0.698+-0.022 | 0.678+-0.033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results = {\n",
    "    'test_accuracy' : [],\n",
    "    'test_precision' : [],\n",
    "    'test_recall' : [],\n",
    "    'test_f1' : []\n",
    "}\n",
    "\n",
    "for j, (train_index, val_index, test_index) in enumerate(tqdm(cv_fold_i)):\n",
    "    \n",
    "    clf_lr_1 = LogisticRegression(max_iter=5000, C=1, penalty='l2', solver='liblinear')\n",
    "    \n",
    "    y_train_t = df['assestment'].values[train_index]\n",
    "    X_train_t = emb_combined[train_index,:]\n",
    "    y_test_t = df['assestment'].values[test_index]\n",
    "    X_test_t = emb_combined[test_index,:]\n",
    "\n",
    "    clf_lr_1.fit(X_train_t, y_train_t)\n",
    "\n",
    "    y_pred = clf_lr_1.predict(X_test_t)\n",
    "    \n",
    "    results['test_accuracy'].append( accuracy_score(y_test_t, y_pred) ) \n",
    "    results['test_precision'].append( precision_score(y_test_t, y_pred) ) \n",
    "    results['test_recall'].append( recall_score(y_test_t, y_pred) ) \n",
    "    results['test_f1'].append( f1_score(y_test_t, y_pred) ) \n",
    "\n",
    "\n",
    "out = {\n",
    "    \"Accuracy\": np.array(results['test_accuracy']),\n",
    "#     \"Precision\": np.array(results['test_precision']).mean(),\n",
    "#     \"Recall\": np.array(results['test_recall']).mean(),\n",
    "    \"F1 Score\":  np.array(results['test_f1']),\n",
    "    }\n",
    "\n",
    "print(\n",
    "    'triplet loss lr C1',\n",
    "    f'Accuracy {out[\"Accuracy\"].mean():.3f}+-{out[\"Accuracy\"].std():.3f}',\n",
    "    f'F1 Score {out[\"F1 Score\"].mean():.3f}+-{out[\"F1 Score\"].std():.3f}',\n",
    "    f' {out[\"Accuracy\"].mean():.3f}+-{out[\"Accuracy\"].std():.3f} | {out[\"F1 Score\"].mean():.3f}+-{out[\"F1 Score\"].std():.3f}'\n",
    ")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 10/10 [00:05<00:00,  1.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "triplet loss lr C1 Accuracy 0.686+-0.015 F1 Score 0.660+-0.049  0.686+-0.015 | 0.660+-0.049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results = {\n",
    "    'test_accuracy' : [],\n",
    "    'test_precision' : [],\n",
    "    'test_recall' : [],\n",
    "    'test_f1' : []\n",
    "}\n",
    "\n",
    "for j, (train_index, val_index, test_index) in enumerate(tqdm(cv_fold_i)):\n",
    "    \n",
    "    clf_lr_1 = LogisticRegression(max_iter=5000, C=1, penalty='l2', solver='liblinear')\n",
    "    \n",
    "    y_train_t = df['assestment'].values[train_index]\n",
    "    X_train_t = emb_combined[train_index,:]\n",
    "    y_test_t = df['assestment'].values[test_index]\n",
    "    X_test_t = emb_combined[test_index,:]\n",
    "    \n",
    "    pca = PCA(n_components=100)\n",
    "    pca.fit(X_train_t)\n",
    "\n",
    "    X_train_t = pca.transform(X_train_t)\n",
    "    X_test_t = pca.transform(X_test_t)\n",
    "\n",
    "    clf_lr_1.fit(X_train_t, y_train_t)\n",
    "\n",
    "    y_pred = clf_lr_1.predict(X_test_t)\n",
    "    \n",
    "    results['test_accuracy'].append( accuracy_score(y_test_t, y_pred) ) \n",
    "    results['test_precision'].append( precision_score(y_test_t, y_pred) ) \n",
    "    results['test_recall'].append( recall_score(y_test_t, y_pred) ) \n",
    "    results['test_f1'].append( f1_score(y_test_t, y_pred) ) \n",
    "\n",
    "\n",
    "out = {\n",
    "    \"Accuracy\": np.array(results['test_accuracy']),\n",
    "#     \"Precision\": np.array(results['test_precision']).mean(),\n",
    "#     \"Recall\": np.array(results['test_recall']).mean(),\n",
    "    \"F1 Score\":  np.array(results['test_f1']),\n",
    "    }\n",
    "\n",
    "print(\n",
    "    'triplet loss lr C1',\n",
    "    f'Accuracy {out[\"Accuracy\"].mean():.3f}+-{out[\"Accuracy\"].std():.3f}',\n",
    "    f'F1 Score {out[\"F1 Score\"].mean():.3f}+-{out[\"F1 Score\"].std():.3f}',\n",
    "    f' {out[\"Accuracy\"].mean():.3f}+-{out[\"Accuracy\"].std():.3f} | {out[\"F1 Score\"].mean():.3f}+-{out[\"F1 Score\"].std():.3f}'\n",
    ")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 10/10 [00:18<00:00,  1.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "triplet loss lr C1 Accuracy 0.695+-0.013 F1 Score 0.679+-0.019  0.695+-0.013 | 0.679+-0.019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results = {\n",
    "    'test_accuracy' : [],\n",
    "    'test_precision' : [],\n",
    "    'test_recall' : [],\n",
    "    'test_f1' : []\n",
    "}\n",
    "\n",
    "for j, (train_index, val_index, test_index) in enumerate(tqdm(cv_Kfold_i)):\n",
    "    \n",
    "    clf_lr_1 = LogisticRegression(max_iter=5000, C=1, penalty='l2', solver='liblinear')\n",
    "    \n",
    "    y_train_t = df['assestment'].values[train_index]\n",
    "    X_train_t = emb_combined[train_index,:]\n",
    "    y_test_t = df['assestment'].values[test_index]\n",
    "    X_test_t = emb_combined[test_index,:]\n",
    "\n",
    "    clf_lr_1.fit(X_train_t, y_train_t)\n",
    "\n",
    "    y_pred = clf_lr_1.predict(X_test_t)\n",
    "    \n",
    "    results['test_accuracy'].append( accuracy_score(y_test_t, y_pred) ) \n",
    "    results['test_precision'].append( precision_score(y_test_t, y_pred) ) \n",
    "    results['test_recall'].append( recall_score(y_test_t, y_pred) ) \n",
    "    results['test_f1'].append( f1_score(y_test_t, y_pred) ) \n",
    "\n",
    "\n",
    "out = {\n",
    "    \"Accuracy\": np.array(results['test_accuracy']),\n",
    "#     \"Precision\": np.array(results['test_precision']).mean(),\n",
    "#     \"Recall\": np.array(results['test_recall']).mean(),\n",
    "    \"F1 Score\":  np.array(results['test_f1']),\n",
    "    }\n",
    "\n",
    "print(\n",
    "    'triplet loss lr C1',\n",
    "    f'Accuracy {out[\"Accuracy\"].mean():.3f}+-{out[\"Accuracy\"].std():.3f}',\n",
    "    f'F1 Score {out[\"F1 Score\"].mean():.3f}+-{out[\"F1 Score\"].std():.3f}',\n",
    "    f' {out[\"Accuracy\"].mean():.3f}+-{out[\"Accuracy\"].std():.3f} | {out[\"F1 Score\"].mean():.3f}+-{out[\"F1 Score\"].std():.3f}'\n",
    ")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 10/10 [00:03<00:00,  2.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "triplet loss lr C1 Accuracy 0.694+-0.013 F1 Score 0.681+-0.017  0.694+-0.013 | 0.681+-0.017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results = {\n",
    "    'test_accuracy' : [],\n",
    "    'test_precision' : [],\n",
    "    'test_recall' : [],\n",
    "    'test_f1' : []\n",
    "}\n",
    "\n",
    "for j, (train_index, val_index, test_index) in enumerate(tqdm(cv_Kfold_i)):\n",
    "    \n",
    "    clf_lr_1 = LogisticRegression(max_iter=5000, C=1, penalty='l2', solver='liblinear')\n",
    "    \n",
    "    y_train_t = df['assestment'].values[train_index]\n",
    "    X_train_t = emb_combined[train_index,:]\n",
    "    y_test_t = df['assestment'].values[test_index]\n",
    "    X_test_t = emb_combined[test_index,:]\n",
    "    \n",
    "    pca = PCA(n_components=100)\n",
    "    pca.fit(X_train_t)\n",
    "\n",
    "    X_train_t = pca.transform(X_train_t)\n",
    "    X_test_t = pca.transform(X_test_t)\n",
    "\n",
    "    clf_lr_1.fit(X_train_t, y_train_t)\n",
    "\n",
    "    y_pred = clf_lr_1.predict(X_test_t)\n",
    "    \n",
    "    results['test_accuracy'].append( accuracy_score(y_test_t, y_pred) ) \n",
    "    results['test_precision'].append( precision_score(y_test_t, y_pred) ) \n",
    "    results['test_recall'].append( recall_score(y_test_t, y_pred) ) \n",
    "    results['test_f1'].append( f1_score(y_test_t, y_pred) ) \n",
    "\n",
    "\n",
    "out = {\n",
    "    \"Accuracy\": np.array(results['test_accuracy']),\n",
    "#     \"Precision\": np.array(results['test_precision']).mean(),\n",
    "#     \"Recall\": np.array(results['test_recall']).mean(),\n",
    "    \"F1 Score\":  np.array(results['test_f1']),\n",
    "    }\n",
    "\n",
    "print(\n",
    "    'triplet loss lr C1',\n",
    "    f'Accuracy {out[\"Accuracy\"].mean():.3f}+-{out[\"Accuracy\"].std():.3f}',\n",
    "    f'F1 Score {out[\"F1 Score\"].mean():.3f}+-{out[\"F1 Score\"].std():.3f}',\n",
    "    f' {out[\"Accuracy\"].mean():.3f}+-{out[\"Accuracy\"].std():.3f} | {out[\"F1 Score\"].mean():.3f}+-{out[\"F1 Score\"].std():.3f}'\n",
    ")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
